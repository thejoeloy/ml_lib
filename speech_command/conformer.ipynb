{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c6587c-5fe8-427b-99f7-0ba0bb69111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thejoey/.local/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset for MFCC\n",
    "class CustomSpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, subset_dataset):\n",
    "        self.subset_dataset = subset_dataset\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, label, speaker_id, utterance_number = self.subset_dataset[index]\n",
    "        mfcc = self.mfcc_transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        return mfcc, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset_dataset)\n",
    "\n",
    "# Initialize the dataset\n",
    "root_path = './'\n",
    "speech_commands_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root_path, download=False)\n",
    "\n",
    "# Split the dataset\n",
    "total_size = len(speech_commands_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = random_split(speech_commands_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create Custom Dataset for MFCC\n",
    "train_set_mfcc = CustomSpeechCommandsDataset(train_set)\n",
    "val_set_mfcc = CustomSpeechCommandsDataset(val_set)\n",
    "test_set_mfcc = CustomSpeechCommandsDataset(test_set)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# List of words in the dataset\n",
    "words = [\n",
    "    \"Backward\", \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Down\", \"Eight\", \"Five\",\n",
    "    \"Follow\", \"Forward\", \"Four\", \"Go\", \"Happy\", \"House\", \"Learn\", \"Left\",\n",
    "    \"Marvin\", \"Nine\", \"No\", \"Off\", \"On\", \"One\", \"Right\", \"Seven\", \"Sheila\",\n",
    "    \"Six\", \"Stop\", \"Three\", \"Tree\", \"Two\", \"Up\", \"Visual\", \"Wow\", \"Yes\", \"Zero\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to map labels to integers\n",
    "label_to_int = {word.lower(): i for i, word in enumerate(words)}\n",
    "\n",
    "# Update collate function to handle string labels\n",
    "def collate_fn(batch):\n",
    "    mfccs, labels = zip(*batch)\n",
    "    mfccs = pad_sequence(mfccs, batch_first=True)\n",
    "    labels = torch.Tensor([label_to_int[label.lower()] for label in labels]).long()\n",
    "    return mfccs, labels\n",
    "\n",
    "# Initialize DataLoaders with the new collate function\n",
    "train_loader = DataLoader(train_set_mfcc, batch_size=128, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set_mfcc, batch_size=128, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set_mfcc, batch_size=128, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84a7bae-06d6-49a8-b5da-68d2475d0916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Intermediate Training Loss: 2.3573558163642883\n",
      "Epoch 1, Batch 200, Intermediate Training Loss: 1.7296855649352074\n",
      "Epoch 1, Batch 300, Intermediate Training Loss: 1.4111414967974028\n",
      "Epoch 1, Batch 400, Intermediate Training Loss: 1.217072047367692\n",
      "Epoch 1, Batch 500, Intermediate Training Loss: 1.083378227829933\n",
      "Epoch 1, Batch 600, Intermediate Training Loss: 0.9862859053909778\n",
      "Epoch 1, Final Training Loss: 0.936647387384648, Validation Loss: 0.48994318297110406\n",
      "Validation Accuracy: 85.62653562653563%\n",
      "Epoch 2, Batch 100, Intermediate Training Loss: 0.4270892927050591\n",
      "Epoch 2, Batch 200, Intermediate Training Loss: 0.4224137419462204\n",
      "Epoch 2, Batch 300, Intermediate Training Loss: 0.41093695213397347\n",
      "Epoch 2, Batch 400, Intermediate Training Loss: 0.4029212475568056\n",
      "Epoch 2, Batch 500, Intermediate Training Loss: 0.39622187420725824\n",
      "Epoch 2, Batch 600, Intermediate Training Loss: 0.3872585309793552\n",
      "Epoch 2, Final Training Loss: 0.38277460639210265, Validation Loss: 0.3253150422530002\n",
      "Validation Accuracy: 90.38934038934039%\n",
      "Epoch 3, Batch 100, Intermediate Training Loss: 0.32048519715666773\n",
      "Epoch 3, Batch 200, Intermediate Training Loss: 0.31738212674856187\n",
      "Epoch 3, Batch 300, Intermediate Training Loss: 0.3154423578083515\n",
      "Epoch 3, Batch 400, Intermediate Training Loss: 0.319156024903059\n",
      "Epoch 3, Batch 500, Intermediate Training Loss: 0.31656036093831064\n",
      "Epoch 3, Batch 600, Intermediate Training Loss: 0.3183378212402264\n",
      "Epoch 3, Final Training Loss: 0.31462000422967523, Validation Loss: 0.2727513893182019\n",
      "Validation Accuracy: 92.31714231714233%\n",
      "Epoch 4, Batch 100, Intermediate Training Loss: 0.2868166981637478\n",
      "Epoch 4, Batch 200, Intermediate Training Loss: 0.28831855379045007\n",
      "Epoch 4, Batch 300, Intermediate Training Loss: 0.290542303075393\n",
      "Epoch 4, Batch 400, Intermediate Training Loss: 0.2896845767088234\n",
      "Epoch 4, Batch 500, Intermediate Training Loss: 0.2876915118247271\n",
      "Epoch 4, Batch 600, Intermediate Training Loss: 0.28611256284018355\n",
      "Epoch 4, Final Training Loss: 0.28308847289971356, Validation Loss: 0.3026612476412072\n",
      "Validation Accuracy: 90.83349083349084%\n",
      "Epoch 5, Batch 100, Intermediate Training Loss: 0.26165009699761865\n",
      "Epoch 5, Batch 200, Intermediate Training Loss: 0.27303344193845985\n",
      "Epoch 5, Batch 300, Intermediate Training Loss: 0.27008373248080414\n",
      "Epoch 5, Batch 400, Intermediate Training Loss: 0.26787250183522704\n",
      "Epoch 5, Batch 500, Intermediate Training Loss: 0.26720683300495146\n",
      "Epoch 5, Batch 600, Intermediate Training Loss: 0.26635190695524213\n",
      "Epoch 5, Final Training Loss: 0.26574130725464434, Validation Loss: 0.23413409095212637\n",
      "Validation Accuracy: 93.05424305424306%\n",
      "Epoch 6, Batch 100, Intermediate Training Loss: 0.25028256841003893\n",
      "Epoch 6, Batch 200, Intermediate Training Loss: 0.2526214992627501\n",
      "Epoch 6, Batch 300, Intermediate Training Loss: 0.2506411495804787\n",
      "Epoch 6, Batch 400, Intermediate Training Loss: 0.2480282999202609\n",
      "Epoch 6, Batch 500, Intermediate Training Loss: 0.24918461142480372\n",
      "Epoch 6, Batch 600, Intermediate Training Loss: 0.2521924759820104\n",
      "Epoch 6, Final Training Loss: 0.25231266009348396, Validation Loss: 0.3228607522435935\n",
      "Validation Accuracy: 90.42714042714043%\n",
      "Epoch 7, Batch 100, Intermediate Training Loss: 0.22596903525292875\n",
      "Epoch 7, Batch 200, Intermediate Training Loss: 0.2334605623409152\n",
      "Epoch 7, Batch 300, Intermediate Training Loss: 0.23648764898379643\n",
      "Epoch 7, Batch 400, Intermediate Training Loss: 0.23862740160897375\n",
      "Epoch 7, Batch 500, Intermediate Training Loss: 0.24109992395341395\n",
      "Epoch 7, Batch 600, Intermediate Training Loss: 0.24038048227628073\n",
      "Epoch 7, Final Training Loss: 0.24123971981314374, Validation Loss: 0.22974374531263328\n",
      "Validation Accuracy: 93.29049329049329%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Initialize Conformer model parameters\n",
    "input_dim = 40  # Assuming 40-dimensional MFCC\n",
    "num_heads = 4\n",
    "ffn_dim = 128\n",
    "num_layers = 4  # Number of Conformer blocks\n",
    "dropout_prob = 0.22\n",
    "\n",
    "# Check for GPU availability and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Conformer model and additional classification layer, then move to device\n",
    "num_classes = len(label_to_int)\n",
    "conformer_model = torchaudio.models.Conformer(\n",
    "    input_dim, \n",
    "    num_heads, \n",
    "    ffn_dim,  \n",
    "    num_layers, \n",
    "    depthwise_conv_kernel_size=17,   \n",
    "    dropout=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "classifier_layer = nn.Linear(input_dim, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    list(conformer_model.parameters()) + list(classifier_layer.parameters()), lr=0.008, weight_decay=.02\n",
    ")\n",
    "\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "num_epochs = 7\n",
    "for epoch in range(num_epochs):\n",
    "    conformer_model.train()\n",
    "    classifier_layer.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (mfcc, labels) in enumerate(train_loader):\n",
    "        # Move data and labels to device\n",
    "        mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = conformer_model(\n",
    "            mfcc, torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device)\n",
    "        )\n",
    "        output = classifier_layer(output.mean(dim=1))\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print intermediate training loss every 100 batches\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Batch {i+1}, Intermediate Training Loss: {running_loss / (i+1)}\"\n",
    "            )\n",
    "\n",
    "    # Validation\n",
    "    conformer_model.eval()\n",
    "    classifier_layer.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (mfcc, labels) in enumerate(val_loader):\n",
    "            # Move data and labels to device\n",
    "            mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "\n",
    "            output, _ = conformer_model(\n",
    "                mfcc,\n",
    "                torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device),\n",
    "            )\n",
    "            output = classifier_layer(output.mean(dim=1))\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Count correct predictions for validation accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Final Training Loss: {running_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\"\n",
    "    )\n",
    "    print(f\"Validation Accuracy: {(correct / total) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36c1708-a76c-4eed-9f10-4de03245fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 0.236755488687251\n",
      "Test Accuracy: 92.86659108087679%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       159\n",
      "           1       0.90      0.92      0.91       201\n",
      "           2       0.97      0.89      0.93       209\n",
      "           3       0.89      0.90      0.89       206\n",
      "           4       0.82      0.97      0.89       209\n",
      "           5       0.94      0.91      0.93       375\n",
      "           6       0.95      0.98      0.96       364\n",
      "           7       0.82      0.99      0.89       417\n",
      "           8       0.84      0.86      0.85       146\n",
      "           9       0.91      0.74      0.82       144\n",
      "          10       0.90      0.95      0.93       389\n",
      "          11       0.99      0.85      0.91       372\n",
      "          12       0.85      0.95      0.90       203\n",
      "          13       0.98      0.90      0.94       200\n",
      "          14       0.91      0.86      0.88       188\n",
      "          15       0.90      0.96      0.93       379\n",
      "          16       0.94      0.97      0.96       217\n",
      "          17       0.96      0.90      0.93       393\n",
      "          18       0.94      0.94      0.94       376\n",
      "          19       0.88      0.96      0.92       360\n",
      "          20       0.99      0.85      0.92       399\n",
      "          21       0.92      0.95      0.93       385\n",
      "          22       0.97      0.93      0.95       377\n",
      "          23       0.98      0.97      0.97       413\n",
      "          24       0.92      0.96      0.94       203\n",
      "          25       0.93      0.98      0.95       374\n",
      "          26       0.93      0.98      0.95       381\n",
      "          27       0.92      0.92      0.92       389\n",
      "          28       0.92      0.87      0.90       189\n",
      "          29       0.95      0.96      0.95       381\n",
      "          30       0.93      0.90      0.92       376\n",
      "          31       0.89      0.96      0.92       150\n",
      "          32       0.93      0.91      0.92       218\n",
      "          33       0.99      0.95      0.97       436\n",
      "          34       0.99      0.92      0.96       406\n",
      "\n",
      "    accuracy                           0.93     10584\n",
      "   macro avg       0.93      0.92      0.92     10584\n",
      "weighted avg       0.93      0.93      0.93     10584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "# Initialize lists to store true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Test evaluation\n",
    "conformer_model.eval()  # Set the model to evaluation mode\n",
    "classifier_layer.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (mfcc, labels) in enumerate(test_loader):\n",
    "        # Move data and labels to device\n",
    "        mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = conformer_model(\n",
    "            mfcc, torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device)\n",
    "        )\n",
    "        output = classifier_layer(output.mean(dim=1))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Count correct predictions for test accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store true and predicted labels for classification report\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Final Test Loss: {test_loss / len(test_loader)}\")\n",
    "print(f\"Test Accuracy: {(correct / total) * 100}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(num_classes)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e011e37-0b55-4aab-ac8a-cf72e3e6b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to save the model\n",
    "model_path = \"conformer_model.pth\"\n",
    "classifier_path = \"classifier_layer.pth\"\n",
    "\n",
    "# Save the entire Conformer model\n",
    "torch.save(conformer_model, model_path)\n",
    "\n",
    "# Save only the state dictionary of the Conformer model\n",
    "torch.save(conformer_model.state_dict(), f\"{model_path}_state_dict\")\n",
    "\n",
    "# Save the entire classifier layer\n",
    "torch.save(classifier_layer, classifier_path)\n",
    "\n",
    "# Save only the state dictionary of the classifier layer\n",
    "torch.save(classifier_layer.state_dict(), f\"{classifier_path}_state_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae86d0-c891-44cb-bb35-ac1b090f7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the entire model\n",
    "loaded_conformer_model = torch.load(model_path)\n",
    "loaded_classifier_layer = torch.load(classifier_path)\n",
    "\n",
    "# If you saved the state dictionaries, first initialize the model and classifier\n",
    "# with their respective architectures, and then load the state dictionaries:\n",
    "conformer_model = torchaudio.models.Conformer(\n",
    "    input_dim, num_heads, ffn_dim, num_layers, dropout=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "classifier_layer = nn.Linear(input_dim, num_classes).to(device)\n",
    "\n",
    "conformer_model.load_state_dict(torch.load(f\"{model_path}_state_dict\"))\n",
    "classifier_layer.load_state_dict(torch.load(f\"{classifier_path}_state_dict\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
