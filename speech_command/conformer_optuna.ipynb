{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f05e18-25e5-43b9-8fc2-82e1218eec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thejoey/.local/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset for MFCC\n",
    "class CustomSpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, subset_dataset):\n",
    "        self.subset_dataset = subset_dataset\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, label, speaker_id, utterance_number = self.subset_dataset[index]\n",
    "        mfcc = self.mfcc_transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        return mfcc, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset_dataset)\n",
    "\n",
    "# Initialize the dataset\n",
    "root_path = './'\n",
    "speech_commands_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=root_path, download=False)\n",
    "\n",
    "# Split the dataset\n",
    "total_size = len(speech_commands_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = random_split(speech_commands_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create Custom Dataset for MFCC\n",
    "train_set_mfcc = CustomSpeechCommandsDataset(train_set)\n",
    "val_set_mfcc = CustomSpeechCommandsDataset(val_set)\n",
    "test_set_mfcc = CustomSpeechCommandsDataset(test_set)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# List of words in the dataset\n",
    "words = [\n",
    "    \"Backward\", \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Down\", \"Eight\", \"Five\",\n",
    "    \"Follow\", \"Forward\", \"Four\", \"Go\", \"Happy\", \"House\", \"Learn\", \"Left\",\n",
    "    \"Marvin\", \"Nine\", \"No\", \"Off\", \"On\", \"One\", \"Right\", \"Seven\", \"Sheila\",\n",
    "    \"Six\", \"Stop\", \"Three\", \"Tree\", \"Two\", \"Up\", \"Visual\", \"Wow\", \"Yes\", \"Zero\"\n",
    "]\n",
    "\n",
    "# Create a dictionary to map labels to integers\n",
    "label_to_int = {word.lower(): i for i, word in enumerate(words)}\n",
    "\n",
    "# Update collate function to handle string labels\n",
    "def collate_fn(batch):\n",
    "    mfccs, labels = zip(*batch)\n",
    "    mfccs = pad_sequence(mfccs, batch_first=True)\n",
    "    labels = torch.Tensor([label_to_int[label.lower()] for label in labels]).long()\n",
    "    return mfccs, labels\n",
    "\n",
    "# Initialize DataLoaders with the new collate function\n",
    "train_loader = DataLoader(train_set_mfcc, batch_size=128, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set_mfcc, batch_size=128, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set_mfcc, batch_size=128, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e6b8c-08d2-4b52-989c-172486ac66d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-18 15:30:25,892] A new study created in memory with name: no-name-a67c5931-7412-4fd6-acf7-9de4be9c53a6\n",
      "[I 2023-10-18 15:34:25,655] Trial 0 finished with value: 88.31033831033831 and parameters: {'learning_rate': 0.013522788412423428, 'dropout_prob': 0.28233408393227505, 'weight_decay': 0.08817480045278472}. Best is trial 0 with value: 88.31033831033831.\n",
      "[I 2023-10-18 15:38:52,607] Trial 1 finished with value: 86.60933660933661 and parameters: {'learning_rate': 0.015449809523678238, 'dropout_prob': 0.28126336330952456, 'weight_decay': 0.12963409056935382}. Best is trial 0 with value: 88.31033831033831.\n",
      "[I 2023-10-18 15:43:22,030] Trial 2 finished with value: 89.70893970893971 and parameters: {'learning_rate': 0.011559886236620917, 'dropout_prob': 0.2596110278181936, 'weight_decay': 0.04678156861583808}. Best is trial 2 with value: 89.70893970893971.\n",
      "[I 2023-10-18 15:47:45,084] Trial 3 finished with value: 87.74333774333775 and parameters: {'learning_rate': 0.009287523294841362, 'dropout_prob': 0.26713167000842614, 'weight_decay': 0.1273783758715295}. Best is trial 2 with value: 89.70893970893971.\n",
      "[I 2023-10-18 15:50:58,597] Trial 4 finished with value: 74.66452466452466 and parameters: {'learning_rate': 0.01551610770723336, 'dropout_prob': 0.26125832722112385, 'weight_decay': 0.17405229097802927}. Best is trial 2 with value: 89.70893970893971.\n",
      "[I 2023-10-18 15:51:39,128] Trial 5 pruned. \n",
      "[I 2023-10-18 15:54:58,484] Trial 6 finished with value: 90.8996408996409 and parameters: {'learning_rate': 0.0086784450696777, 'dropout_prob': 0.26762281709407076, 'weight_decay': 0.08074733681629152}. Best is trial 6 with value: 90.8996408996409.\n",
      "[I 2023-10-18 15:55:36,861] Trial 7 pruned. \n",
      "[I 2023-10-18 15:56:15,608] Trial 8 pruned. \n",
      "[I 2023-10-18 15:56:53,914] Trial 9 pruned. \n",
      "[I 2023-10-18 15:59:52,496] Trial 10 finished with value: 90.03024003024002 and parameters: {'learning_rate': 0.008055674399025348, 'dropout_prob': 0.2883313137236519, 'weight_decay': 0.015195872879112476}. Best is trial 6 with value: 90.8996408996409.\n",
      "[I 2023-10-18 16:02:36,004] Trial 11 finished with value: 92.22264222264222 and parameters: {'learning_rate': 0.008175205167349157, 'dropout_prob': 0.2898675871111901, 'weight_decay': 0.027259409173411048}. Best is trial 11 with value: 92.22264222264222.\n",
      "[I 2023-10-18 16:03:08,467] Trial 12 pruned. \n",
      "[I 2023-10-18 16:04:12,653] Trial 13 pruned. \n",
      "[I 2023-10-18 16:04:45,503] Trial 14 pruned. \n",
      "[I 2023-10-18 16:05:18,517] Trial 15 pruned. \n",
      "[I 2023-10-18 16:05:51,434] Trial 16 pruned. \n",
      "[I 2023-10-18 16:08:40,673] Trial 17 finished with value: 89.36873936873937 and parameters: {'learning_rate': 0.00800765572232557, 'dropout_prob': 0.2577632373426729, 'weight_decay': 0.07797435483571255}. Best is trial 11 with value: 92.22264222264222.\n",
      "[I 2023-10-18 16:09:17,515] Trial 18 pruned. \n",
      "[I 2023-10-18 16:10:29,084] Trial 19 pruned. \n",
      "[I 2023-10-18 16:13:50,655] Trial 20 finished with value: 90.88074088074089 and parameters: {'learning_rate': 0.010021566933881721, 'dropout_prob': 0.276838563601041, 'weight_decay': 0.0355371944992075}. Best is trial 11 with value: 92.22264222264222.\n",
      "[I 2023-10-18 16:14:35,344] Trial 21 pruned. \n",
      "[I 2023-10-18 16:15:23,375] Trial 22 pruned. \n",
      "[I 2023-10-18 16:16:48,686] Trial 23 pruned. \n",
      "[I 2023-10-18 16:20:45,645] Trial 24 finished with value: 91.55169155169155 and parameters: {'learning_rate': 0.00939312407152702, 'dropout_prob': 0.27078747104619166, 'weight_decay': 0.007635033246906356}. Best is trial 11 with value: 92.22264222264222.\n",
      "[I 2023-10-18 16:21:26,465] Trial 25 pruned. \n",
      "[I 2023-10-18 16:22:03,395] Trial 26 pruned. \n",
      "[I 2023-10-18 16:22:45,536] Trial 27 pruned. \n",
      "[I 2023-10-18 16:23:26,917] Trial 28 pruned. \n",
      "[I 2023-10-18 16:24:08,889] Trial 29 pruned. \n",
      "[I 2023-10-18 16:24:47,917] Trial 30 pruned. \n",
      "[I 2023-10-18 16:25:36,438] Trial 31 pruned. \n",
      "[I 2023-10-18 16:26:27,540] Trial 32 pruned. \n",
      "[I 2023-10-18 16:27:12,201] Trial 33 pruned. \n",
      "[I 2023-10-18 16:31:23,191] Trial 34 finished with value: 90.58779058779058 and parameters: {'learning_rate': 0.008851362759177151, 'dropout_prob': 0.27017057875048905, 'weight_decay': 0.0451241627448198}. Best is trial 11 with value: 92.22264222264222.\n",
      "[I 2023-10-18 16:35:49,457] Trial 35 finished with value: 92.88414288414289 and parameters: {'learning_rate': 0.008399010630851855, 'dropout_prob': 0.28161257327102224, 'weight_decay': 0.018590021076214103}. Best is trial 35 with value: 92.88414288414289.\n",
      "[I 2023-10-18 16:36:38,465] Trial 36 pruned. \n",
      "[I 2023-10-18 16:41:07,719] Trial 37 finished with value: 91.43829143829144 and parameters: {'learning_rate': 0.008464811484274117, 'dropout_prob': 0.2819469648089782, 'weight_decay': 0.05154781494997718}. Best is trial 35 with value: 92.88414288414289.\n",
      "[I 2023-10-18 16:41:58,540] Trial 38 pruned. \n",
      "[I 2023-10-18 16:46:32,223] Trial 39 finished with value: 92.51559251559252 and parameters: {'learning_rate': 0.009111425352383462, 'dropout_prob': 0.2800480475554694, 'weight_decay': 0.011379286504529674}. Best is trial 35 with value: 92.88414288414289.\n",
      "[I 2023-10-18 16:47:34,899] Trial 40 pruned. \n",
      "[I 2023-10-18 16:48:24,889] Trial 41 pruned. \n",
      "[I 2023-10-18 16:49:16,958] Trial 42 pruned. \n",
      "[I 2023-10-18 16:50:19,165] Trial 43 pruned. \n",
      "[I 2023-10-18 16:51:10,927] Trial 44 pruned. \n",
      "[I 2023-10-18 16:52:11,713] Trial 45 pruned. \n",
      "[I 2023-10-18 16:56:23,399] Trial 46 finished with value: 92.2131922131922 and parameters: {'learning_rate': 0.009239154088127081, 'dropout_prob': 0.28832761577076244, 'weight_decay': 0.04041711937085775}. Best is trial 35 with value: 92.88414288414289.\n",
      "[I 2023-10-18 16:57:16,475] Trial 47 pruned. \n",
      "[I 2023-10-18 16:58:08,290] Trial 48 pruned. \n",
      "[I 2023-10-18 16:59:07,640] Trial 49 pruned. \n",
      "[I 2023-10-18 16:59:57,079] Trial 50 pruned. \n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "input_dim = 40  # Assuming 40-dimensional MFCC\n",
    "# Initialize Conformer model parameters\n",
    "num_heads = 4\n",
    "ffn_dim = 128\n",
    "num_layers = 4  # Number of Conformer blocks\n",
    "# Check for GPU availability and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize Conformer model and additional classification layer, then move to device\n",
    "num_classes = len(label_to_int)\n",
    "# Function to be optimized\n",
    "def objective(trial):\n",
    "    \n",
    "    # Hyperparameters to be optimized\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.008, 0.016)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.24, 0.29)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.005, 0.2)\n",
    "    \n",
    "    # Initialize Conformer model and additional classification layer, then move to device\n",
    "    conformer_model = torchaudio.models.Conformer(\n",
    "        input_dim, \n",
    "        num_heads, \n",
    "        ffn_dim,  \n",
    "        num_layers, \n",
    "        depthwise_conv_kernel_size=17,   \n",
    "        dropout=dropout_prob\n",
    "    ).to(device)\n",
    "\n",
    "    classifier_layer = nn.Linear(input_dim, num_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        list(conformer_model.parameters()) + list(classifier_layer.parameters()), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        conformer_model.train()\n",
    "        classifier_layer.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (mfcc, labels) in enumerate(train_loader):\n",
    "            mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = conformer_model(\n",
    "                mfcc, torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device)\n",
    "            )\n",
    "            output = classifier_layer(output.mean(dim=1))\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        conformer_model.eval()\n",
    "        classifier_layer.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (mfcc, labels) in enumerate(val_loader):\n",
    "                mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "                output, _ = conformer_model(\n",
    "                    mfcc, torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device),\n",
    "                )\n",
    "                output = classifier_layer(output.mean(dim=1))\n",
    "                loss = criterion(output, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = (correct / total) * 100\n",
    "        trial.report(val_accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a1868-7cca-4587-b47a-617cb26cc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "# Initialize lists to store true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Test evaluation\n",
    "conformer_model.eval()  # Set the model to evaluation mode\n",
    "classifier_layer.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (mfcc, labels) in enumerate(test_loader):\n",
    "        # Move data and labels to device\n",
    "        mfcc, labels = mfcc.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = conformer_model(\n",
    "            mfcc, torch.full((mfcc.size(0),), mfcc.size(1), dtype=torch.long).to(device)\n",
    "        )\n",
    "        output = classifier_layer(output.mean(dim=1))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Count correct predictions for test accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store true and predicted labels for classification report\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Final Test Loss: {test_loss / len(test_loader)}\")\n",
    "print(f\"Test Accuracy: {(correct / total) * 100}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(num_classes)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
