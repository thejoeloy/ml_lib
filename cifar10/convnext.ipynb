{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61dba2-c95c-40b0-a657-d70a5f79f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def unpickle(file): \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Directory where the CIFAR-10 batches are stored\n",
    "data_dir = 'data/cifar-10-batches-py'\n",
    "\n",
    "# List to store all data batches\n",
    "data_list = []\n",
    "\n",
    "# List to store all labels from batches\n",
    "labels_list = []\n",
    "\n",
    "# There are 5 data batches named data_batch_1, data_batch_2, ..., data_batch_5\n",
    "for i in range(1, 6):\n",
    "    file_path = os.path.join(data_dir, f'data_batch_{i}')\n",
    "    batch_data = unpickle(file_path)\n",
    "    data_list.append(batch_data[b'data'])\n",
    "    labels_list.extend(batch_data[b'labels'])\n",
    "\n",
    "# Convert the list of data batches to a single numpy array\n",
    "data_array = np.vstack(data_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "data_tensor = data_tensor / 255.0\n",
    "\n",
    "# Reshape the data_tensor to (num_samples, channels, height, width)\n",
    "data_tensor = data_tensor.view(-1, 3, 32, 32)\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data_tensor, labels_tensor, transform=None):\n",
    "        self.data = data_tensor\n",
    "        self.labels = labels_tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert tensor to PIL Image\n",
    "        sample = to_pil_image(sample)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "# Define the recommended transformations for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # CIFAR-10 normalization\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),  # CIFAR-10 normalization\n",
    "])\n",
    "\n",
    "test_file_path = 'data/cifar-10-batches-py/test_batch'\n",
    "test_batch_data = unpickle(test_file_path)\n",
    "test_data_array = np.array(test_batch_data[b'data'])\n",
    "test_labels_array = np.array(test_batch_data[b'labels'])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "test_data_tensor = torch.tensor(test_data_array, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels_array, dtype=torch.long)\n",
    "\n",
    "# Normalize and reshape\n",
    "test_data_tensor = test_data_tensor / 255.0\n",
    "test_data_tensor = test_data_tensor.view(-1, 3, 32, 32)\n",
    "# Create the custom dataset\n",
    "train_dataset = CIFAR10Dataset(data_tensor, labels_tensor, transform=transform)\n",
    "test_dataset = CIFAR10Dataset(test_data_tensor, test_labels_tensor, transform=test_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b89825-8fae-42e4-971f-09dc7422d23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.1040\n",
      "Epoch [1/3], Loss: 1.9875\n",
      "Epoch [1/3], Loss: 1.8964\n",
      "Epoch [1/3], Loss: 1.8115\n",
      "Epoch [1/3], Loss: 1.7343\n",
      "Epoch [1/3], Loss: 1.6545\n",
      "Epoch [1/3], Loss: 1.5915\n",
      "Epoch [1/3], Loss: 1.5257\n",
      "Epoch [1/3], Loss: 1.4609\n",
      "Epoch [1/3], Loss: 1.4075\n",
      "Epoch [1/3], Loss: 1.3561\n",
      "Epoch [1/3], Loss: 1.3046\n",
      "Epoch [1/3], Loss: 1.2629\n",
      "Epoch [1/3], Loss: 1.2230\n",
      "Epoch [1/3], Loss: 1.1848\n",
      "Epoch [1/3], Loss: 1.1499\n",
      "Epoch [1/3], Loss: 1.1177\n",
      "Epoch [1/3], Loss: 1.0885\n",
      "Epoch [1/3], Loss: 1.0614\n",
      "Epoch [1/3], Loss: 1.0354\n",
      "Epoch [1/3], Loss: 1.0124\n",
      "Epoch [1/3], Loss: 0.9884\n",
      "Epoch [1/3], Loss: 0.9663\n",
      "Epoch [1/3], Loss: 0.9450\n",
      "Epoch [1/3], Loss: 0.9256\n",
      "Epoch [1/3], Loss: 0.9076\n",
      "Epoch [1/3], Loss: 0.8906\n",
      "Epoch [1/3], Loss: 0.8733\n",
      "Epoch [1/3], Loss: 0.8576\n",
      "Epoch [1/3], Loss: 0.8418\n",
      "Epoch [1/3], Loss: 0.8274\n",
      "Epoch [2/3], Loss: 0.3825\n",
      "Epoch [2/3], Loss: 0.4006\n",
      "Epoch [2/3], Loss: 0.3755\n",
      "Epoch [2/3], Loss: 0.3619\n",
      "Epoch [2/3], Loss: 0.3542\n",
      "Epoch [2/3], Loss: 0.3486\n",
      "Epoch [2/3], Loss: 0.3438\n",
      "Epoch [2/3], Loss: 0.3454\n",
      "Epoch [2/3], Loss: 0.3427\n",
      "Epoch [2/3], Loss: 0.3454\n",
      "Epoch [2/3], Loss: 0.3431\n",
      "Epoch [2/3], Loss: 0.3417\n",
      "Epoch [2/3], Loss: 0.3412\n",
      "Epoch [2/3], Loss: 0.3380\n",
      "Epoch [2/3], Loss: 0.3372\n",
      "Epoch [2/3], Loss: 0.3364\n",
      "Epoch [2/3], Loss: 0.3334\n",
      "Epoch [2/3], Loss: 0.3302\n",
      "Epoch [2/3], Loss: 0.3281\n",
      "Epoch [2/3], Loss: 0.3267\n",
      "Epoch [2/3], Loss: 0.3253\n",
      "Epoch [2/3], Loss: 0.3228\n",
      "Epoch [2/3], Loss: 0.3215\n",
      "Epoch [2/3], Loss: 0.3198\n",
      "Epoch [2/3], Loss: 0.3178\n",
      "Epoch [2/3], Loss: 0.3160\n",
      "Epoch [2/3], Loss: 0.3141\n",
      "Epoch [2/3], Loss: 0.3118\n",
      "Epoch [2/3], Loss: 0.3109\n",
      "Epoch [2/3], Loss: 0.3093\n",
      "Epoch [2/3], Loss: 0.3075\n",
      "Epoch [3/3], Loss: 0.1880\n",
      "Epoch [3/3], Loss: 0.1918\n",
      "Epoch [3/3], Loss: 0.2071\n",
      "Epoch [3/3], Loss: 0.2113\n",
      "Epoch [3/3], Loss: 0.2145\n",
      "Epoch [3/3], Loss: 0.2162\n",
      "Epoch [3/3], Loss: 0.2155\n",
      "Epoch [3/3], Loss: 0.2178\n",
      "Epoch [3/3], Loss: 0.2143\n",
      "Epoch [3/3], Loss: 0.2125\n",
      "Epoch [3/3], Loss: 0.2145\n",
      "Epoch [3/3], Loss: 0.2128\n",
      "Epoch [3/3], Loss: 0.2131\n",
      "Epoch [3/3], Loss: 0.2128\n",
      "Epoch [3/3], Loss: 0.2127\n",
      "Epoch [3/3], Loss: 0.2109\n",
      "Epoch [3/3], Loss: 0.2097\n",
      "Epoch [3/3], Loss: 0.2105\n",
      "Epoch [3/3], Loss: 0.2118\n",
      "Epoch [3/3], Loss: 0.2130\n",
      "Epoch [3/3], Loss: 0.2124\n",
      "Epoch [3/3], Loss: 0.2113\n",
      "Epoch [3/3], Loss: 0.2113\n",
      "Epoch [3/3], Loss: 0.2104\n",
      "Epoch [3/3], Loss: 0.2102\n",
      "Epoch [3/3], Loss: 0.2094\n",
      "Epoch [3/3], Loss: 0.2084\n",
      "Epoch [3/3], Loss: 0.2081\n",
      "Epoch [3/3], Loss: 0.2073\n",
      "Epoch [3/3], Loss: 0.2063\n",
      "Epoch [3/3], Loss: 0.2052\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import convnext_base  # Importing ConvNeXt Base model\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "# Modify the ConvNeXt Base model to fit the CIFAR-10 dataset\n",
    "model = convnext_base(weights='DEFAULT')  \n",
    "\n",
    "model.features[0] = nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00009, weight_decay=0.015)\n",
    "\n",
    "# Check for GPU availability and move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):  # Assuming train_dataloader is defined\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (i+1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0555e20-6e8a-440e-bcbd-2c0495a08c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.98      0.96      0.97      1000\n",
      "  automobile       0.96      0.98      0.97      1000\n",
      "        bird       0.97      0.95      0.96      1000\n",
      "         cat       0.91      0.91      0.91      1000\n",
      "        deer       0.96      0.97      0.96      1000\n",
      "         dog       0.91      0.93      0.92      1000\n",
      "        frog       0.97      0.98      0.98      1000\n",
      "       horse       0.99      0.96      0.98      1000\n",
      "        ship       0.98      0.98      0.98      1000\n",
      "       truck       0.97      0.96      0.97      1000\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "Overall Accuracy: 96.04%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Evaluation on test data with classification report\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Append batch predictions and true labels\n",
    "        y_true += labels.cpu().numpy().tolist()\n",
    "        y_pred += predicted.cpu().numpy().tolist()\n",
    "\n",
    "# Compute and print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']))\n",
    "# Compute and print the overall accuracy\n",
    "overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4666b74b-d41f-43e1-8c43-4cd85f32e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved to cifar10_convnext.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'cifar10_convnext.pth')\n",
    "print(\"Model has been saved to cifar10_convnext.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22099373-ce43-4d8e-aa4c-1dfa855cd8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
